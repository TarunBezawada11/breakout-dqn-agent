# ğŸ® Deep Q-Network (DQN) for Atari Breakout  
**CS6482 â€“ Deep Reinforcement Learning Project**

Train an AI agent to master Atari Breakout using advanced reinforcement learning techniques like **Dueling DQN** and **Prioritized Experience Replay (PER)**.

---

## ğŸš€ Project Highlights

- ğŸ“ˆ Achieved **average reward of 21.67** by version **v10**
- ğŸ§  Implemented **Vanilla DQN**, **Dueling DQN**, and **PER**
- ğŸ–¼ï¸ Preprocessed game frames using **OpenCV** and **scikit-image**
- ğŸ§ª Trained the agent over **5,000 episodes** using Q-learning with a CNN-based policy network
- âš™ï¸ Training performed on **NVIDIA A100 GPU** for high-throughput learning

---

## ğŸ› ï¸ Technologies Used

| Tool/Library     | Purpose                                       |
|------------------|-----------------------------------------------|
| **PyTorch**       | Deep Q-Network and CNN implementation         |
| **OpenAI Gym**    | Atari Breakout environment (ALE/Breakout-v5) |
| **ALE-Py**        | Support for Atari Learning Environment        |
| **OpenCV**        | Frame preprocessing (grayscale, resize)      |
| **Scikit-Image**  | Advanced image transformations                |
| **NumPy**         | Numerical computations                        |
| **Matplotlib**    | Reward/loss visualization                     |

---

## ğŸ§  Skills Demonstrated

- **Reinforcement Learning**  
  Q-learning with Bellman updates, Îµ-greedy exploration, and experience replay

- **Deep Learning**  
  Designed a 5-layer CNN to process 84Ã—84Ã—4 grayscale stacked frames

- **Optimization**  
  Used Prioritized Experience Replay (PER) and fine-tuned learning rate (0.0002)

- **Data Engineering**  
  Frame preprocessing, stacking, and efficient buffer management

- **Evaluation**  
  Plotted reward curves and Q-loss to track learning progression

- **Software Engineering**  
  Modular, version-controlled codebase (tracked from v1 â†’ v10)

---

## ğŸ“Š Results Summary

| Version | Method         | Avg. Reward |
|---------|----------------|-------------|
| v1      | Vanilla DQN    | 1.90        |
| v5      | Dueling DQN    | 12.5 â€“ 15   |
| v10     | DQN + PER      | **21.67**   |

ğŸ•’ **Training Time**: ~14.5 hours on NVIDIA A100 GPU

---

## ğŸ“‰ Visual Insights

### ğŸ“ˆ Reward Curve  
Demonstrates the agentâ€™s learning progress and long-term performance.

### ğŸ“‰ Loss Curve  
Indicates the convergence behavior of Q-values during training.
https://github.com/TarunBezawada11/breakout-dqn-agent/blob/main/rewards_curve.png
## ğŸ’» Compute & Infrastructure

- **GPU**: NVIDIA A100 (80GB HBM2e)
- **Replay Buffer**: ~1M transitions
- **Input**: 84Ã—84Ã—4 frame stacks
- **Model**: CNN with millions of parameters, using target network updates

---

## ğŸ”§ Getting Started

```bash
# Clone the repository
git clone https://github.com/BlachThunder6302/CS6482-DQN-Breakout.git

# Install dependencies
pip install -r requirements.txt

# Launch the notebook
jupyter notebook CS6482-DQN-Atari.ipynb
